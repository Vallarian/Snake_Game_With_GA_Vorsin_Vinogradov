import numpy as np

# Определяем размеры матриц весов для нейронной сети
n_x = 7  # количество нейронов в входном слое
n_h = 9  # количество нейронов в первом скрытом слое
n_h2 = 15  # количество нейронов во втором скрытом слое
n_y = 3  # количество нейронов в выходном слое

# Формы матриц весов между слоями
W1_shape = (n_h, n_x)  # форма матрицы весов между входным и первым скрытым слоем
W2_shape = (n_h2, n_h)  # форма матрицы весов между первым и вторым скрытым слоем
W3_shape = (n_y, n_h2)  # форма матрицы весов между вторым скрытым и выходным слоем


# Функция для извлечения матриц весов из закодированного индивидуального представления
def get_weights_from_encoded(individual):
    # Разбиваем индивидуальный массив на части, соответствующие матрицам весов
    W1 = individual[0:W1_shape[0] * W1_shape[1]]  # извлекаем веса для первого слоя
    W2 = individual[W1_shape[0] * W1_shape[1]:W2_shape[0] * W2_shape[1] + W1_shape[0] * W1_shape[1]]  # извлекаем веса для второго слоя
    W3 = individual[W2_shape[0] * W2_shape[1] + W1_shape[0] * W1_shape[1]:]  # извлекаем веса для третьего слоя

    # Возвращаем матрицы весов в нужной форме
    return (W1.reshape(W1_shape[0], W1_shape[1]),  # преобразуем в матрицу формы W1_shape
        W2.reshape(W2_shape[0], W2_shape[1]),  # преобразуем в матрицу формы W2_shape
        W3.reshape(W3_shape[0], W3_shape[1])) # преобразуем в матрицу формы W3_shape


# Функция softmax для нормализации выходных значений сети
def softmax(z):
    # Вычисляем экспоненту каждого элемента и делим на сумму экспонент по строкам
    s = np.exp(z.T) / np.sum(np.exp(z.T), axis=1).reshape(-1, 1)
    return s


# Функция сигмоиды для активации нейронов
def sigmoid(z):
    # Вычисляем сигмоид для каждого элемента
    s = 1 / (1 + np.exp(-z))
    return s


# Функция прямого распространения для вычисления выходных значений сети
def forward_propagation(X, individual):
    # Извлекаем матрицы весов из индивидуального представления
    W1, W2, W3 = get_weights_from_encoded(individual)

    # Вычисляем значения для каждого слоя
    Z1 = np.matmul(W1, X.T)  # вычисляем взвешенную сумму для первого скрытого слоя
    A1 = np.tanh(Z1)  # применяем функцию активации tanh
    Z2 = np.matmul(W2, A1)  # вычисляем взвешенную сумму для второго скрытого слоя
    A2 = np.tanh(Z2)  # применяем функцию активации tanh
    Z3 = np.matmul(W3, A2)  # вычисляем взвешенную сумму для выходного слоя
    A3 = softmax(Z3)  # применяем функцию softmax для нормализации выходных значений

    return A3  # возвращаем выходные значения сети